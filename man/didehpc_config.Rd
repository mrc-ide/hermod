% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/config.R
\name{didehpc_config}
\alias{didehpc_config}
\title{Configuration}
\usage{
didehpc_config(
  credentials = NULL,
  home = NULL,
  temp = NULL,
  cluster = NULL,
  shares = NULL,
  template = NULL,
  cores = NULL,
  wholenode = NULL,
  r_version = NULL,
  use_java = NULL,
  java_home = NULL,
  workdir = NULL
)
}
\arguments{
\item{credentials}{Either a list with elements username, password,
or a path to a file containing lines \verb{username=<username>}
and \verb{password=<password>} or your username (in which case
you will be prompted graphically for your password).}

\item{home}{Path to network home directory, on local system}

\item{temp}{Path to network temp directory, on local system}

\item{cluster}{Name of the cluster to use; one of
\code{\link[didehpc:valid_clusters]{didehpc::valid_clusters()}} or one of the aliases
(small/little/dide/ide; big/mrc).}

\item{shares}{Optional additional share mappings.  Can either be a
single path mapping (as returned by \code{\link[didehpc:path_mapping]{didehpc::path_mapping()}}
or a list of such calls.}

\item{template}{A job template.  On fi--dideclusthn this can be
"GeneralNodes" or "8Core". On "fi--didemrchnb" this can be
"GeneralNodes", "12Core", "16Core", "12and16Core", "20Core",
"24Core", "32Core", or "MEM1024" (for nodes with 1Tb of RAM; we have
three - two of which have 32 cores, and the other is the AMD epyc with
64). On the new "wpia-hn" cluster, you should
currently use "AllNodes". See the main cluster documentation if you
tweak these parameters, as you may not have permission to use
all templates (and if you use one that you don't have permission
for the job will fail).  For training purposes there is also a
"Training" template, but you will only need to use this when
instructed to.}

\item{cores}{The number of cores to request.  If specified, then
we will request this many cores from the windows queuer.  If you
request too many cores then your task will queue forever!  24 is
the largest this can be on fi--dideclusthn. On fi--didemrchnb,
the GeneralNodes template has mainly 20 cores or less, with a
single 64 core node, and the 32Core template has 32 core
nodes. On wpia-hn, all the nodes are 32 core. If \code{cores} is
omitted then a single core is assumed, unless \code{wholenode} is
TRUE.}

\item{wholenode}{If TRUE, request exclusive access to whichever
compute node is allocated to the job. Your code will have access
to all the cores and memory on the node.}

\item{r_version}{A string, or \code{numeric_version} object, describing
the R version required.  Not all R versions are known to be
supported, so this will check against a list of installed R
versions for the cluster you are using.  If omitted then: if
your R version matches a version on the cluster that will be
used, or the oldest cluster version that is newer than yours, or
the most recent cluster version.}

\item{use_java}{Logical, indicating if the script is going to
require Java, for example via the rJava package.}

\item{java_home}{A string, optionally giving the path of a
custom Java Runtime Environment, which will be used if
the use_java logical is true. If left blank, then the
default cluster Java Runtime Environment will be used.}

\item{workdir}{Here be dragons.}
}
\description{
Collects configuration information.
}
